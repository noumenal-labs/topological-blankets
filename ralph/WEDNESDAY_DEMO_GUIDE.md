# Wednesday Demo: Teleoperation + TB-Guided Planning

## What This Demo Shows

The Wednesday demo runs a narrated six-phase scenario on FetchPush-v4,
illustrating the full pipeline from autonomous operation through human-in-the-loop
intervention and back to autonomous completion. It produces a single
presentation-ready GIF with phase banners, imagined trajectory overlays, and
catastrophe signal indicators.

The six narrative phases:

1. **Autonomous Push** -- The pre-trained ensemble solves the push task with low uncertainty.
2. **Novel Configuration** -- A perturbation displaces the object to an unfamiliar position.
3. **Uncertainty Spike** -- Ensemble members disagree; the catastrophe signal fires. Imagined trajectories visualize prediction divergence.
4. **Human Takeover** -- A human operator injects corrective subgoals via the teleop interface.
5. **Collaborative Completion** -- The task completes with human-agent collaboration.
6. **Structure Analysis** -- TB coupling matrices compare pre- vs. post-perturbation structure.

---

## How Human Interventions Work

Previously, the demo had a bug where human-injected goals were computed but
*never actually used* to control the gripper (random noise was sent to the
environment instead). This has been fixed. The interventions are now visible
and directly responsible for task completion.

### The Teleop Architecture

```
Human Operator
    |
    |  inject_goal(target_xyz)      <- injects a 3D workspace coordinate
    v
TeleopInterface
    |
    |  decide(obs, achieved, desired)  <- returns SymbolicDecision
    v                                     with a PlanningObjective
Proportional Controller
    |
    |  action = clip((target - gripper_pos) * gain, action_bounds)
    v
FetchPush Environment
```

The `TeleopInterface` sits between the human operator and the planner. In
*agent mode*, it delegates to whatever planner is active (symbolic or
TB-guided). In *human mode*, it overrides the planner with a
`PlanningObjective` that drives the gripper toward the injected target
position.

The proportional controller then translates that objective into environment
actions:

```python
delta = objective.gripper_target - current_gripper_position
action[:3] = clip(delta * 10.0, action_low, action_high)
```

This means each human-injected goal *visibly moves the gripper* toward that
position. The gain of 10.0 produces responsive but stable tracking within the
FetchPush action bounds.

### What Each Intervention Does in the Demo

The demo scripts two goal injections:

1. **First goal** (step 32): `[1.30, 0.75, 0.44]` -- positions the gripper
   *above* the displaced object, correcting for the perturbation that knocked
   the object off its expected trajectory.

2. **Second goal** (step 42): `[1.35, 0.82, 0.42]` -- moves the gripper to
   the push-approach position behind the object along the goal direction,
   setting up a clean push.

After step 52, control returns to the autonomous planner, which drives the push
to completion. The agent now has 28 steps after handback (previously only 20,
which was insufficient).

### Why Interventions Now Visibly Help

Three bugs were fixed:

| Bug | Symptom | Fix |
|-----|---------|-----|
| Random actions sent to env | Gripper wandered randomly regardless of human goals | Proportional controller follows `decision.objective.gripper_target` |
| Missing "second_goal" key frame | Only 1 goal injection counted; `goal_injections_ge_2` failed | Added `second_goal_step` check in live-mode key-frame detection |
| Not enough post-handback steps | 60 total steps, release at 40, only 20 to finish | Increased to 80 steps; release at 52, giving 28 steps |

---

## Imagined Trajectories and the Teleop Story

Each ensemble member maintains its own dynamics model, producing a distinct
prediction of the gripper's future path. We call these per-member rollouts
*imagined trajectories* (also referred to as counterfactual trajectories in
the planning literature). The demo renders them as an overlay during phases 2
through 4: each imagined trajectory appears as a semi-transparent colored line
on the trajectory panel. The overlay is generated by `_overlay_ghosts()` in
`wednesday_demo.py` and included in every frame of the main GIF (no separate
file needed).

### What the imagined trajectories communicate

- During **Phase 1** (autonomous), trajectories are tightly clustered, conveying
  that the ensemble agrees on what will happen next.
- During **Phase 3** (uncertainty spike), trajectories diverge widely, visually
  showing the ensemble's disagreement. This is the moment the catastrophe
  signal fires and motivates the handoff.
- During **Phase 4** (human takeover), trajectories show moderate spread, reflecting
  partial re-convergence as the human steers toward a corrective position.

### Why this matters for the latency/miscalibration narrative

The key point for external audiences: the human does not intervene on *where
the robot is right now*. The human intervenes on *where the robot expects to
go*. The imagined trajectories make this visible. When latency or
miscalibration exists in teleoperation, acting on the robot's current state
leads to oscillation and lag. Acting on the robot's *projected future* (the
ensemble's t+1 prediction) absorbs that latency. The trajectory overlay is the
visual evidence that the system exposes its predictions to the operator, and
the operator's injected goals redirect those predictions rather than
fighting them.

This is already integrated into the single demo GIF. No separate visualization
is needed, though individual trajectory frames can be extracted from the
key-frame PNGs saved to `ralph/results/`.

---

## Epistemic Actions and the Info-Gain Bonus

The current demo does *not* use epistemic actions (the information-gain
exploration bonus is turned off). This is intentional for the teleop
demonstration, where the narrative focuses on human intervention rather than
autonomous exploration.

However, the infrastructure fully supports it. The CEM planner
(`panda/planner.py`) has an `epistemic_bonus_weight` parameter in `CEMConfig`:

```python
@dataclass(frozen=True)
class CEMConfig:
    ...
    epistemic_bonus_weight: float   # 0.0 = off, > 0.0 = info-gain exploration
    ...
```

When `epistemic_bonus_weight > 0`, the planner adds ensemble disagreement as a
bonus to the CEM objective. This causes the agent to *seek out* states where
the ensemble disagrees, i.e., states where it can learn the most. The
`evaluate_sequences()` function already computes
`epistemic_bonus_weight * ensemble_disagreement` and adds it to the planning
score.

### Where to demonstrate epistemic actions

The FetchReach task is the cleanest environment for showing the benefit of
epistemic actions, because it has no high-level planner and reward alone may
not drive efficient exploration under sparse reward. A comparison matrix:

| Condition | Sparse Reward | Dense Reward |
|-----------|--------------|--------------|
| Without info-gain | Slow to discover goal region | Reaches goal, no exploration benefit |
| With info-gain | Actively explores, finds goal faster | Mild benefit (reward already informative) |

The clearest signal is *sparse reward with vs. without info-gain*. The existing
lunar-lander demo already shows this benefit in a different environment; the
robotics version would strengthen the story by showing the same principle
transfers.

Note: FetchReach is relatively easy even with sparse reward, so the effect may
be subtle. FetchPush with sparse reward is harder and would show a larger gap,
but requires the high-level planner (symbolic or TB-guided) to decompose the
task. The `--planner tb` mode makes this benchmarkable without hardcoded task
knowledge.

---

## Benchmarking Strategy

### What can be independently validated

The system supports two categories of comparison that an external evaluator
could reproduce:

1. **Model-free baseline (e.g. SAC):** Compare sample efficiency. The ensemble
   world model should reach equivalent performance in far fewer environment
   interactions than a model-free method like Soft Actor-Critic. This is the
   standard "model-based vs. model-free" argument and is straightforward to
   set up using any standard SAC implementation on the same task.

2. **Ablation of uncertainty components:** Compare the same ensemble agent
   *with* vs. *without* the uncertainty-driven parts (epistemic bonus,
   catastrophe signal, imagined trajectories). This is a more subtle comparison
   but arguably more important; it isolates the contribution of our
   uncertainty-aware architecture rather than just showing "world models are
   more sample efficient," which is well-established.

### FetchReach vs. FetchPush for benchmarking

| Task | Benchmarkable? | Notes |
|------|---------------|-------|
| FetchReach | Yes, fully apples-to-apples | No high-level planner needed. TB-guided planner produces a single-phase reach plan automatically. All methods see the same problem. |
| FetchPush | Yes, with `--planner tb` | The TB-guided planner learns the approach-then-push decomposition from structure rather than hardcoding it. This makes the comparison fair, since the decomposition is *discovered* not given. Without `--planner tb`, the symbolic planner's hardcoded knowledge is an unfair advantage. |

A practical presentation strategy: show *benchmarks on FetchReach* (clean,
apples-to-apples, no decomposition needed) and the *handoff demo on
FetchPush* (richer narrative, demonstrates teleop + structure discovery).
This separates the quantitative claim (sample efficiency, uncertainty
calibration) from the qualitative story (human-in-the-loop intervention,
TB-guided planning).

### The OpenAI DDPG+HER baseline

The strongest existing baseline for FetchPush is DDPG with Hindsight Experience
Replay (Andrychowicz et al., 2017). The official OpenAI results show:

- Median success rate approaching 1.0 within ~50 epochs
- ~5-8 million total environment steps (19 MPI workers)
- DDPG *without* HER fails completely on FetchPush with sparse reward
- W&B logs for the official runs are available (Alec has downloaded these)

The comparison should log the same metrics (mean reward per env step, mean
success rate per env step) and plot side by side.

### Environment version note (v1 vs v4)

The OpenAI baselines were run on FetchPush-v1; the current codebase uses v4.
The task definition (observation space, action space, reward function, MuJoCo
XML model) is identical across versions. The differences are:

- v1 â†’ v2: migrated from `mujoco_py` to DeepMind's `mujoco` bindings
- v3: fixed a bug where `env.reset()` did not properly reset internal state
- v4: fixed initial state to match documented specification

Results are *qualitatively* comparable (the task is the same), but the reset
bug means v1 initial-state distributions may differ slightly from v4. For a
rigorous comparison, the DDPG+HER baseline should ideally be re-run on v4, or
at minimum the version difference should be noted.

### Selecting comparison methods

Two good choices:

- *DDPG+HER* (model-free, the established SOTA for FetchPush). Official W&B
  logs available for direct comparison. This is the strongest apples-to-apples
  baseline since it was specifically tuned for these tasks.
- A *recently prominent* method: DreamerV3 or TD-MPC2 (model-based, well-known
  in the 2024-2025 literature). This shows that the uncertainty-aware
  components add value even relative to other world-model approaches.

For either comparison, publicly available implementations on Gymnasium robotics
tasks exist, which makes reproducibility straightforward. The key is that each
method needs some task-specific tuning, so starting with methods that have
existing FetchReach/FetchPush results or readily available configs is practical.

---

## Running the Demo

### Prerequisites

The demo has two modes with different dependency requirements:

**Dry-run mode** (no external dependencies beyond numpy and matplotlib):
```bash
pip install numpy matplotlib imageio
```

**Live mode** (requires MuJoCo and the pandas ensemble codebase):
```bash
pip install gymnasium[mujoco] gymnasium-robotics imageio
# The pandas repo must be on sys.path (the script handles this automatically
# if the repo is at the expected location: ../../../pandas relative to the
# script, or at C:/Users/citiz/Documents/noumenal-labs/pandas)
```

For the `--planner tb-discover` mode, the `topological_blankets` package and a
trained JAX ensemble checkpoint are also needed:
```bash
pip install jax jaxlib equinox
```

If `gymnasium` is not installed, the script automatically falls back to dry-run
mode and prints a notice.

### Basic (dry-run, no MuJoCo needed)

```bash
cd topological-blankets/ralph
python experiments/wednesday_demo.py --dry-run
```

This uses synthetic data and produces a GIF demonstrating the full narrative
without requiring a trained model or MuJoCo installation. The synthetic mode
generates plausible gripper/object trajectories, uncertainty curves, and
imagined trajectory overlays so the visual output closely resembles a live run.

Expected console output:
```
=================================================================
  Wednesday Demo -- End-to-End Narrated Scenario
=================================================================
  Mode:           DRY-RUN (synthetic)
  Planner:        symbolic
  Max steps:      80
  ...

Episode complete: 80 steps, 80 frames in 12.3s
Saving outputs...
  GIF saved: ralph/results/20260210_143022_wednesday_demo.gif
  GIF duration: 40.0s (80 frames at 2 fps)
  Key frame PNGs: 8 saved

Acceptance Criteria:
  [PASS] all_phases_covered
  [PASS] catastrophe_within_5
  [PASS] goal_injections_ge_2
  [PASS] task_completes
  [PASS] gif_duration_ok
  [PASS] has_saved_files
```

### Live Mode (requires gymnasium-robotics)

```bash
python experiments/wednesday_demo.py
```

Runs on the actual FetchPush-v4 environment with the symbolic planner. The
script creates the environment, loads any available trained model, and runs a
single episode with scripted perturbation and human goal injections. Rendering
uses MuJoCo's offscreen renderer (EGL on headless Linux, native on macOS/Windows).

To use a specific trained checkpoint:
```bash
# The script looks for a model at pandas/data/push_demo/model.eqx by default.
# If no checkpoint exists, the CEM planner still runs but with a randomly
# initialized ensemble (useful for testing the pipeline, though task
# completion is unlikely).
```

### With TB-Guided Planner

```bash
# Ground-truth TB partition (no trained model needed)
python experiments/wednesday_demo.py --planner tb

# Full discovery pipeline (requires trained ensemble + TB package)
python experiments/wednesday_demo.py --planner tb-discover
```

The `--planner` flag selects the planning backend:

| Mode | Description | Requirements |
|------|-------------|--------------|
| `symbolic` (default) | Hardcoded approach-then-push decomposition | `panda.symbolic_planner` |
| `tb` | TB-guided planner using the known FetchPush variable partition | `panda.learned_planner` |
| `tb-discover` | Full pipeline: collect frames, compute Jacobian sensitivities from ensemble, run TB discovery, build planner | `panda.tb_discovery` + trained ensemble + `topological_blankets` package |

All three modes produce the same demo narrative and are evaluated against the
same six acceptance criteria. The difference is *how* the planner decomposes
the task internally.

The `tb-discover` mode prints additional output showing the discovery pipeline:
```
[tb_discovery] Collecting 10000 random frames...
[tb_discovery] Computing Jacobian sensitivities...
[tb_discovery] Running TB discovery on 2500 gradient samples (25D)...
[tb_discovery] Discovered 2 objects, 3 blanket variables.
```

If the TB package or trained model is unavailable, `tb-discover` falls back to
the ground-truth partition automatically (equivalent to `--planner tb`).

### Adjusting Episode Length

The default is 80 steps. Phase boundaries scale proportionally when
`--max-steps` is changed:

```bash
# Shorter episode (faster, but agent has less time to complete)
python experiments/wednesday_demo.py --max-steps 60

# Longer episode (more time for autonomous completion after handback)
python experiments/wednesday_demo.py --max-steps 120
```

The script prints the computed phase boundaries at startup so there is no
ambiguity about when perturbation, goal injection, and release occur.

### CLI Options

```
--dry-run          Use synthetic data (no environment)
--max-steps N      Total episode steps (default: 80)
--seed N           Random seed (default: 42)
--gif-fps N        GIF frame rate (default: 2)
--dpi N            Output resolution (default: 100)
--planner MODE     Planner: symbolic | tb | tb-discover
```

### Output Files

Each run produces a timestamped set of files in `ralph/results/`:

| File | Description |
|------|-------------|
| `YYYYMMDD_HHMMSS_wednesday_demo.gif` | The presentation-ready GIF with phase banners, imagined trajectory overlays, and catastrophe indicators. Duration is `max_steps / gif_fps` seconds. |
| `YYYYMMDD_HHMMSS_wednesday_demo.json` | Results JSON containing per-step metrics, acceptance criteria results, episode configuration, and file paths. |
| `YYYYMMDD_HHMMSS_wednesday_autonomous_push_step000.png` | Key-frame PNG at the start of Phase 1. |
| `YYYYMMDD_HHMMSS_wednesday_perturbation_stepNNN.png` | Key-frame PNG at the perturbation event. |
| `YYYYMMDD_HHMMSS_wednesday_catastrophe_stepNNN.png` | Key-frame PNG when the catastrophe signal fires. |
| `YYYYMMDD_HHMMSS_wednesday_human_takeover_stepNNN.png` | Key-frame PNG at the first human goal injection. |
| `YYYYMMDD_HHMMSS_wednesday_second_goal_stepNNN.png` | Key-frame PNG at the second human goal injection. |
| `YYYYMMDD_HHMMSS_wednesday_release_stepNNN.png` | Key-frame PNG when control returns to the agent. |
| `YYYYMMDD_HHMMSS_wednesday_structure_stepNNN.png` | Key-frame PNG showing the TB coupling matrix comparison. |

The JSON file is structured for programmatic consumption. Key fields:

```json
{
  "config": { "max_steps": 80, "seed": 42, "planner": "symbolic", ... },
  "metrics": {
    "final_goal_distance": 0.087,
    "catastrophe_step": 22,
    "n_goal_injections": 2,
    "autonomy_fraction": 0.75
  },
  "acceptance_criteria": {
    "all_phases_covered": true,
    "catastrophe_within_5": true,
    "goal_injections_ge_2": true,
    "task_completes": true,
    "gif_duration_ok": true,
    "has_saved_files": true
  },
  "saved_files": [ ... ]
}
```

### Troubleshooting

| Symptom | Likely Cause | Fix |
|---------|-------------|-----|
| `ModuleNotFoundError: gymnasium` | MuJoCo not installed | Run with `--dry-run`, or install `gymnasium[mujoco] gymnasium-robotics` |
| Script falls back to dry-run unexpectedly | `gymnasium` import fails silently | Check that `pip install gymnasium[mujoco]` succeeded; on Linux, MuJoCo may need `libGL` |
| `task_completes` fails | Agent cannot reach goal in remaining steps | Try `--max-steps 120` for more post-handback time; or check that the trained checkpoint exists |
| `gif_duration_ok` fails | GIF too short or too long | Adjust `--gif-fps` (lower = longer GIF) or `--max-steps` |
| `catastrophe_within_5` fails | Perturbation too mild for the trained model | This can happen with a well-trained model that handles the perturbation gracefully; try a different `--seed` |
| Black frames in GIF | MuJoCo offscreen rendering failed | On headless Linux, set `MUJOCO_GL=egl`; the script attempts this automatically |
| `[tb_discovery] No model or env provided` | Using `--planner tb-discover` without a trained model | Expected; the pipeline falls back to ground-truth partition. Train a checkpoint first (see pandas `train.py`) |

---

## Acceptance Criteria

The demo verifies six criteria automatically at the end of each run:

| Criterion | Description | What Triggers a FAIL |
|-----------|-------------|----------------------|
| `all_phases_covered` | All 6 narrative phases appear in the episode | A phase was skipped (usually means `max_steps` is too low) |
| `catastrophe_within_5` | Red catastrophe signal fires within 5 steps of perturbation | The ensemble is too confident about the perturbed state, or the perturbation is too mild |
| `goal_injections_ge_2` | At least 2 human goal injections recorded as key frames | The second goal injection was not detected as a key frame |
| `task_completes` | Final goal distance < 0.15 | The agent could not push the object close enough to the goal after handback |
| `gif_duration_ok` | GIF duration between 15 and 120 seconds | Adjust `--gif-fps` or `--max-steps` |
| `has_saved_files` | At least one output file saved | Disk write failed, or the `ralph/results/` directory does not exist |

All six should PASS. The results JSON is saved to `ralph/results/` alongside
the GIF and key-frame PNGs.

---

## TB-Guided Planner: How It Replaces the Hardcoded Planner

The symbolic planner (`panda/symbolic_planner.py`) hardcodes the FetchPush task
decomposition: "approach from behind the object, then push." It knows which
variables are the gripper and which are the object because those indices are
written into the code.

The TB-guided planner (`panda/learned_planner.py`) discovers this same
decomposition from structure. Given a `TBPartition` that assigns observation
variables to objects and a blanket:

1. It infers *causal ordering* from coupling strengths (the object with higher
   outgoing coupling to blanket variables is the "upstream" actuator, i.e. the
   gripper).
2. For a two-object system, it produces the same two-phase plan (approach, then
   push) without any task-specific knowledge.
3. For a single-object system (e.g. FetchReach), it produces a single-phase
   reach plan automatically.

The `tb_discovery.py` module handles the pipeline for *discovering* the partition
from a trained ensemble model:

```
Random frames  -->  Jacobian sensitivities  -->  TopologicalBlankets.fit()  -->  TBPartition
                    (column norms of                                              |
                     d(delta_pred)/d(obs))                                        v
                                                                          TBGuidedPlanner
```

When `--planner tb` is used, the ground-truth FetchPush partition is provided
directly (useful for testing the planner logic without needing a trained model).
When `--planner tb-discover` is used, the full pipeline runs, with graceful
fallback to ground-truth if any component is unavailable.

---

## File Inventory

| File | Purpose |
|------|---------|
| `topological-blankets/ralph/experiments/wednesday_demo.py` | Main demo script |
| `pandas/panda/teleop_interface.py` | TeleopInterface: human goal injection layer |
| `pandas/panda/symbolic_planner.py` | Hardcoded FetchPush symbolic planner |
| `pandas/panda/learned_planner.py` | TB-guided planner (task-agnostic) |
| `pandas/panda/tb_discovery.py` | Jacobian sensitivity + TB discovery pipeline |
| `pandas/panda/planner.py` | CEM planner, `CEMConfig`, `epistemic_bonus_weight` |
| `pandas/panda/catastrophe_bridge.py` | Catastrophe signal evaluation |
| `topological-blankets/topological_blankets/core.py` | TopologicalBlankets class |

---

## Next Steps

- Wire the trained ensemble model into `--planner tb-discover` so it runs the
  full Jacobian discovery pipeline rather than falling back to ground truth.
- Add a telecorder connector so goal injection comes from the physical teleop
  interface rather than scripted coordinates.
- Implement a real-time remote control mode so human teleoperators can control
  the arm live (not just via scripted coordinates). The proportional controller
  already tracks injected goals at 10Hz-equivalent gain; the missing piece is a
  streaming input adapter (e.g. gamepad, spacemouse, or telecorder UI) that
  continuously calls `inject_goal()` with updated target positions.
- Run the FetchReach benchmark suite: with/without `epistemic_bonus_weight`,
  sparse vs. dense reward, compared against SAC and optionally DreamerV3.
- Consider an attention overlay showing *which variables* the TB partition
  identified as the blanket, rendered on the demo GIF in real time.
- Prepare a "pop quiz" package: the benchmark environment and configs packaged
  so an external evaluator can run comparisons on their own hardware.
