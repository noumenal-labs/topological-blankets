{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Blankets: From Energy Landscapes to Markov Blanket Structure\n",
    "\n",
    "This notebook demonstrates the **Topological Blankets (TB)** method, a geometric approach to discovering discrete Markov blanket structure from continuous energy landscape geometry.\n",
    "\n",
    "The key insight: *every energy-based model defines a Riemannian manifold whose curvature encodes conditional independence.* By analyzing Hessian structure (second-order geometry of the energy surface), TB recovers which variables form coherent \"objects\" and which serve as statistical boundaries (Markov blankets) between them.\n",
    "\n",
    "**Outline:**\n",
    "1. Theory Overview\n",
    "2. Synthetic Validation (Quadratic EBM)\n",
    "3. Bridge Experiments (2D Score Model)\n",
    "4. World Model Demo (LunarLander Active Inference + Dreamer)\n",
    "5. Multi-Scale Comparison\n",
    "6. Edge-Compute Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project root\n",
    "ROOT = Path(os.path.abspath('')).parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "RESULTS = ROOT / 'results'\n",
    "FIGURES = ROOT / 'paper' / 'figures'\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 11,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "print(f'Project root: {ROOT}')\n",
    "print(f'Results directory: {RESULTS}')\n",
    "print(f'Available result files: {len(list(RESULTS.glob(\"*.json\")))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Theory Overview\n",
    "\n",
    "### What is a Markov Blanket?\n",
    "\n",
    "A **Markov blanket** is the minimal set of variables that renders a target variable conditionally independent of all others. In a Gaussian graphical model with precision matrix $\\Theta$, the blanket of variable $i$ is exactly $\\{j : \\Theta_{ij} \\neq 0\\}$.\n",
    "\n",
    "### The Energy Landscape Connection\n",
    "\n",
    "For an energy-based model $p(x) \\propto e^{-E(x)}$, the Hessian of the energy encodes coupling structure:\n",
    "\n",
    "$$H_{ij} = \\frac{\\partial^2 E}{\\partial x_i \\partial x_j}$$\n",
    "\n",
    "If $H_{ij} = 0$, variables $i$ and $j$ are conditionally independent given the rest. TB estimates this Hessian from gradient samples and uses spectral analysis to discover block structure.\n",
    "\n",
    "### The TB Pipeline\n",
    "\n",
    "1. **Collect gradient samples**: Either compute $\\nabla E(x)$ analytically or via autodiff\n",
    "2. **Estimate the Hessian**: $\\hat{H} = \\text{Cov}(\\nabla E)$ (gradient covariance)\n",
    "3. **Normalize coupling**: $C_{ij} = |H_{ij}| / \\sqrt{H_{ii} H_{jj}}$ (correlation-like)\n",
    "4. **Detect blankets**: Separate variables into internal (object) and boundary (blanket) roles\n",
    "5. **Cluster objects**: Spectral clustering on the internal-variable subgraph\n",
    "\n",
    "The result is a partition of all variables into discrete objects separated by Markov blankets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Synthetic Validation: Quadratic EBM\n",
    "\n",
    "The simplest test case: a quadratic energy $E(x) = \\frac{1}{2} x^T \\Theta x$ with a known block-diagonal precision matrix. The ground truth partition is known exactly, so recovery can be measured precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topological_blankets import TopologicalBlankets\n",
    "\n",
    "# Build a block-structured precision matrix\n",
    "# 2 objects (3 vars each) + 3 blanket variables = 9 total\n",
    "n_objects = 2\n",
    "vars_per_object = 3\n",
    "vars_per_blanket = 3\n",
    "intra_strength = 6.0    # Strong within-object coupling\n",
    "blanket_strength = 0.8  # Weak cross-coupling via blanket\n",
    "\n",
    "n_vars = n_objects * vars_per_object + vars_per_blanket\n",
    "Theta = np.zeros((n_vars, n_vars))\n",
    "\n",
    "# Strong within-object couplings\n",
    "for i in range(n_objects):\n",
    "    s = i * vars_per_object\n",
    "    e = s + vars_per_object\n",
    "    Theta[s:e, s:e] = intra_strength\n",
    "    np.fill_diagonal(Theta[s:e, s:e], intra_strength * vars_per_object)\n",
    "\n",
    "# Blanket block\n",
    "b_start = n_objects * vars_per_object\n",
    "Theta[b_start:, b_start:] = 1.0\n",
    "np.fill_diagonal(Theta[b_start:, b_start:], vars_per_blanket)\n",
    "\n",
    "# Weak cross-couplings through blanket only\n",
    "for i in range(n_objects):\n",
    "    s = i * vars_per_object\n",
    "    e = s + vars_per_object\n",
    "    Theta[s:e, b_start:] = blanket_strength\n",
    "    Theta[b_start:, s:e] = blanket_strength\n",
    "\n",
    "Theta = (Theta + Theta.T) / 2.0\n",
    "eigvals = np.linalg.eigvalsh(Theta)\n",
    "if eigvals.min() < 0.1:\n",
    "    Theta += np.eye(n_vars) * (0.2 - eigvals.min())\n",
    "\n",
    "# Ground truth\n",
    "gt = np.full(n_vars, -1)  # -1 = blanket\n",
    "for i in range(n_objects):\n",
    "    gt[i*vars_per_object:(i+1)*vars_per_object] = i\n",
    "\n",
    "print(f'Precision matrix shape: {Theta.shape}')\n",
    "print(f'Ground truth: {gt}')\n",
    "print(f'  Object 0: vars {list(np.where(gt==0)[0])}')\n",
    "print(f'  Object 1: vars {list(np.where(gt==1)[0])}')\n",
    "print(f'  Blanket:  vars {list(np.where(gt==-1)[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample via Langevin dynamics and collect gradients\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "step_size = 0.005\n",
    "temp = 0.1\n",
    "\n",
    "samples, gradients = [], []\n",
    "x = np.random.randn(n_vars)\n",
    "\n",
    "for i in range(n_samples * 50):\n",
    "    grad = Theta @ x\n",
    "    noise = np.sqrt(2 * step_size * temp) * np.random.randn(n_vars)\n",
    "    x = x - step_size * grad + noise\n",
    "    if i % 50 == 0:\n",
    "        samples.append(x.copy())\n",
    "        gradients.append((Theta @ x).copy())\n",
    "\n",
    "samples = np.array(samples)\n",
    "gradients = np.array(gradients)\n",
    "print(f'Collected {samples.shape[0]} samples with {gradients.shape[1]}-dim gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Topological Blankets\n",
    "tb = TopologicalBlankets(method='gradient', n_objects=2)\n",
    "tb.fit(gradients)\n",
    "\n",
    "objects = tb.get_objects()\n",
    "blankets = tb.get_blankets()\n",
    "coupling = tb.get_coupling_matrix()\n",
    "assignment = tb.get_assignment()\n",
    "\n",
    "print('TB Result:')\n",
    "for obj_id, vars_idx in objects.items():\n",
    "    print(f'  Object {obj_id}: vars {list(vars_idx)}')\n",
    "print(f'  Blanket: vars {list(blankets)}')\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import adjusted_rand_score, f1_score\n",
    "internal_mask = gt >= 0\n",
    "ari = adjusted_rand_score(gt[internal_mask], assignment[internal_mask])\n",
    "blanket_f1 = f1_score((gt == -1).astype(int), (assignment == -1).astype(int))\n",
    "print(f'\\nObject ARI: {ari:.3f}')\n",
    "print(f'Blanket F1: {blanket_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coupling matrix and partition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# Panel 1: Precision matrix (ground truth)\n",
    "ax = axes[0]\n",
    "im = ax.imshow(np.abs(Theta), cmap='YlOrRd', aspect='auto')\n",
    "ax.set_title('Precision Matrix $|\\\\Theta|$ (Ground Truth)')\n",
    "ax.set_xlabel('Variable j')\n",
    "ax.set_ylabel('Variable i')\n",
    "for b in [vars_per_object, 2*vars_per_object]:\n",
    "    ax.axhline(y=b-0.5, color='white', linewidth=2, linestyle='--')\n",
    "    ax.axvline(x=b-0.5, color='white', linewidth=2, linestyle='--')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Panel 2: TB coupling matrix\n",
    "ax = axes[1]\n",
    "im = ax.imshow(coupling, cmap='hot', aspect='auto')\n",
    "ax.set_title('Estimated Coupling $C_{ij}$ (TB)')\n",
    "ax.set_xlabel('Variable j')\n",
    "ax.set_ylabel('Variable i')\n",
    "for b in [vars_per_object, 2*vars_per_object]:\n",
    "    ax.axhline(y=b-0.5, color='cyan', linewidth=2, linestyle='--')\n",
    "    ax.axvline(x=b-0.5, color='cyan', linewidth=2, linestyle='--')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Panel 3: Partition comparison\n",
    "ax = axes[2]\n",
    "var_idx = np.arange(n_vars)\n",
    "colors_gt = ['#3498db' if g==0 else '#e74c3c' if g==1 else '#2ecc71' for g in gt]\n",
    "colors_tb = ['#3498db' if a==0 else '#e74c3c' if a==1 else '#2ecc71' for a in assignment]\n",
    "bar_width = 0.35\n",
    "ax.bar(var_idx - bar_width/2, np.ones(n_vars), bar_width, color=colors_gt, label='Ground Truth', alpha=0.7)\n",
    "ax.bar(var_idx + bar_width/2, np.ones(n_vars), bar_width, color=colors_tb, label='TB Result', alpha=0.7)\n",
    "ax.set_xlabel('Variable Index')\n",
    "ax.set_title(f'Partition Comparison (ARI={ari:.2f}, F1={blanket_f1:.2f})')\n",
    "ax.set_yticks([])\n",
    "# Legend patches\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#3498db', label='Object 0'),\n",
    "                   Patch(facecolor='#e74c3c', label='Object 1'),\n",
    "                   Patch(facecolor='#2ecc71', label='Blanket')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength Sweep: How Robust is Recovery?\n",
    "\n",
    "Precomputed results from sweeping the blanket coupling strength across 7 levels with 10 trials each. TB is compared against two baselines (DMBD-style variational role assignment and AXIOM-style mixture decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed strength sweep\n",
    "sweep_files = sorted(RESULTS.glob('*strength_sweep_10trials.json'))\n",
    "if sweep_files:\n",
    "    with open(sweep_files[-1]) as f:\n",
    "        sweep_data = json.load(f)\n",
    "    metrics = sweep_data['metrics']\n",
    "    \n",
    "    strengths = sorted([float(s) for s in metrics.keys()])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    methods = ['tc', 'dmbd', 'axiom']\n",
    "    labels = ['Topological Blankets', 'DMBD-style', 'AXIOM-style']\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    \n",
    "    for ax, metric_key, ylabel, title in [\n",
    "        (axes[0], 'mean_ari', 'Adjusted Rand Index', 'Object Partition Recovery'),\n",
    "        (axes[1], 'mean_f1', 'F1 Score', 'Blanket Detection'),\n",
    "    ]:\n",
    "        for method, label, color in zip(methods, labels, colors):\n",
    "            means = [metrics[str(s)][method][metric_key] for s in strengths]\n",
    "            std_key = metric_key.replace('mean', 'std')\n",
    "            stds = [metrics[str(s)][method][std_key] for s in strengths]\n",
    "            ax.errorbar(strengths, means, yerr=stds, label=label,\n",
    "                       color=color, marker='o', capsize=3)\n",
    "        ax.set_xlabel('Blanket Strength (coupling)')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary at key strengths\n",
    "    print('Summary at selected strengths:')\n",
    "    for s in [0.1, 0.5, 0.8, 2.0]:\n",
    "        sk = str(s)\n",
    "        if sk in metrics:\n",
    "            tc_ari = metrics[sk]['tc']['mean_ari']\n",
    "            tc_f1 = metrics[sk]['tc']['mean_f1']\n",
    "            print(f'  strength={s}: TB ARI={tc_ari:.3f}, F1={tc_f1:.3f}')\n",
    "else:\n",
    "    print('No strength sweep results found. Run experiments/quadratic_toy_comparison.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Bridge Experiment: 2D Score Model\n",
    "\n",
    "Moving beyond closed-form energies, TB can analyze *learned* score functions. A small MLP is trained via denoising score matching on 2D synthetic datasets, and TB is applied to the learned score field.\n",
    "\n",
    "This demonstrates TB's ability to extract structure from neural network energy landscapes rather than hand-crafted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed 2D score model results\n",
    "score_files = sorted(RESULTS.glob('*score_model_2d.json'))\n",
    "if score_files:\n",
    "    with open(score_files[-1]) as f:\n",
    "        score_data = json.load(f)\n",
    "    \n",
    "    print('2D Score Model Results')\n",
    "    print('=' * 50)\n",
    "    print(f'{\"Dataset\":<12s} {\"Sample ARI\":>10s} {\"Blanket dims\":>14s} {\"Hessian diag\":>14s}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    for name, r in score_data['metrics'].items():\n",
    "        H = np.array(r['hessian_est'])\n",
    "        blanket = np.where(r['variable_blanket'])[0]\n",
    "        print(f'{name:<12s} {r[\"sample_ari\"]:>10.3f} {str(list(blanket)):>14s} '\n",
    "              f'[{H[0,0]:.2f}, {H[1,1]:.2f}]')\n",
    "    \n",
    "    # Show the pre-generated figures\n",
    "    score_figs = sorted(RESULTS.glob('*score_model_2d_score_field_moons.png'))\n",
    "    if score_figs:\n",
    "        from IPython.display import Image, display\n",
    "        print('\\nScore field and Hessian analysis for the moons dataset:')\n",
    "        display(Image(filename=str(score_figs[-1]), width=700))\n",
    "else:\n",
    "    print('No score model results found. Run experiments/score_model_2d.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2D datasets, the Hessian is a $2 \\times 2$ matrix. The off-diagonal coupling indicates how the two spatial dimensions interact under the energy landscape. Higher coupling suggests the dimensions cannot be factored apart; the score field has diagonal structure when the energy decomposes.\n",
    "\n",
    "The sample-level ARI measures how well spectral clustering on score similarity recovers the true cluster labels. This is *not* the same as TB's variable-level analysis, but demonstrates that the learned score field captures meaningful structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. World Model Demo: LunarLander-v3\n",
    "\n",
    "The central application: applying TB to a trained world model's gradient landscape to discover which state variables form coherent subsystems.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- **Environment**: LunarLander-v3 (8D state: x, y, vx, vy, angle, ang_vel, left_leg, right_leg)\n",
    "- **Active Inference agent**: Ensemble of 5 MLPs predicting next-state distributions\n",
    "- **Dreamer autoencoder**: Encoder (8D $\\to$ 64D) + Decoder (64D $\\to$ 8D), trained on reconstruction loss\n",
    "- **Analysis**: TB applied to dynamics gradients in both 8D state space and 64D latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Active Inference TB analysis\n",
    "actinf_files = sorted(RESULTS.glob('*actinf_tb_analysis.json'))\n",
    "if actinf_files:\n",
    "    with open(actinf_files[-1]) as f:\n",
    "        actinf_data = json.load(f)\n",
    "    \n",
    "    state_labels = actinf_data['config']['state_labels']\n",
    "    dynamics = actinf_data['metrics']['dynamics']\n",
    "    \n",
    "    print('Active Inference World Model: TB on 8D State Space')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Dynamics gradient assignment\n",
    "    assign = dynamics['gradient_method']['assignment']\n",
    "    is_blanket = dynamics['gradient_method']['is_blanket']\n",
    "    \n",
    "    obj0 = [state_labels[i] for i, a in enumerate(assign) if a == 0]\n",
    "    obj1 = [state_labels[i] for i, a in enumerate(assign) if a == 1]\n",
    "    blanket = [state_labels[i] for i, b in enumerate(is_blanket) if b]\n",
    "    \n",
    "    print(f'Dynamics gradient analysis:')\n",
    "    print(f'  Object 0: {obj0}')\n",
    "    print(f'  Object 1: {obj1}')\n",
    "    print(f'  Blanket:  {blanket}')\n",
    "    print(f'  Spectral eigengap: {dynamics[\"eigengap\"]:.1f}')\n",
    "    print(f'  Hierarchical levels: {len(dynamics[\"hierarchy\"])}')\n",
    "    \n",
    "    # Coupling matrix visualization\n",
    "    coupling = np.array(dynamics['coupling'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(coupling, cmap='hot', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(8))\n",
    "    ax.set_xticklabels(state_labels, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(8))\n",
    "    ax.set_yticklabels(state_labels)\n",
    "    ax.set_title('Dynamics Coupling Matrix')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    # Color-code the partition\n",
    "    ax = axes[1]\n",
    "    colors = ['#3498db' if a==0 else '#e74c3c' if a==1 else '#2ecc71' for a in assign]\n",
    "    grad_mag = dynamics['grad_magnitude']\n",
    "    bars = ax.bar(range(8), grad_mag, color=colors)\n",
    "    ax.set_xticks(range(8))\n",
    "    ax.set_xticklabels(state_labels, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Mean $|\\\\nabla E|$')\n",
    "    ax.set_title('Gradient Magnitude by Variable')\n",
    "    legend_elements = [Patch(facecolor='#3498db', label='Object 0'),\n",
    "                       Patch(facecolor='#e74c3c', label='Object 1'),\n",
    "                       Patch(facecolor='#2ecc71', label='Blanket')]\n",
    "    ax.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nPhysical interpretation:')\n",
    "    print(f'  Object 0 ({obj0}): Position/contact subsystem')\n",
    "    print(f'  Object 1 ({obj1}): Orientation/velocity subsystem')\n",
    "    print(f'  Blanket ({blanket}): Angular velocity mediates between subsystems')\n",
    "else:\n",
    "    print('No Active Inference results found. Run experiments/world_model_analysis.py first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple gradient landscapes\n",
    "if actinf_files:\n",
    "    landscapes = ['dynamics', 'reward', 'disagreement']\n",
    "    landscape_labels = ['Dynamics', 'Reward', 'Ensemble Disagreement']\n",
    "    \n",
    "    print('Comparison Across Gradient Landscapes')\n",
    "    print('=' * 70)\n",
    "    print(f'{\"Landscape\":<24s} {\"Object 0\":<24s} {\"Object 1\":<24s} {\"Blanket\":<16s}')\n",
    "    print('-' * 70)\n",
    "    \n",
    "    for lname, llabel in zip(landscapes, landscape_labels):\n",
    "        data = actinf_data['metrics'][lname]\n",
    "        a = data['gradient_method']['assignment']\n",
    "        b = data['gradient_method']['is_blanket']\n",
    "        o0 = [state_labels[i] for i, v in enumerate(a) if v == 0]\n",
    "        o1 = [state_labels[i] for i, v in enumerate(a) if v == 1]\n",
    "        bl = [state_labels[i] for i, v in enumerate(b) if v]\n",
    "        print(f'{llabel:<24s} {str(o0):<24s} {str(o1):<24s} {str(bl):<16s}')\n",
    "    \n",
    "    print('\\nDifferent landscapes reveal different structural views of the same system.')\n",
    "    print('Dynamics: ang_vel as sole mediator between positional and orientational subsystems.')\n",
    "    print('Disagreement: y and vy become blanket, reflecting where model uncertainty is highest.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer Autoencoder: TB in 64D Latent Space\n",
    "\n",
    "A Dreamer-style autoencoder maps the 8D state to 64D latent codes. TB applied to reconstruction loss gradients in latent space discovers structure in the *learned representation*, not just the original state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Dreamer TB analysis\ndreamer_files = sorted(RESULTS.glob('*dreamer_tb_analysis.json'))\nif dreamer_files:\n    with open(dreamer_files[-1]) as f:\n        dreamer_data = json.load(f)\n    \n    dm = dreamer_data['metrics']\n    methods = dm.get('methods', dm)  # Handle both structures\n    \n    print('Dreamer Autoencoder: TB on 64D Latent Space')\n    print('=' * 55)\n    print(f'  Latent dimensions:    64')\n    print(f'  Spectral eigengap:    {dm[\"eigengap\"]:.1f}')\n    \n    # Gradient method results\n    grad_m = methods.get('gradient', {})\n    n_blanket_grad = grad_m.get('n_blanket', 0)\n    obj_sizes_grad = grad_m.get('object_sizes', {})\n    n_internal_grad = sum(obj_sizes_grad.values()) if isinstance(obj_sizes_grad, dict) else 0\n    print(f'  Objects (gradient):   {len(obj_sizes_grad)} ({n_internal_grad} dims)')\n    print(f'  Blanket (gradient):   {n_blanket_grad} dims')\n    \n    # Coupling method results\n    coup_m = methods.get('coupling', {})\n    n_blanket_coup = coup_m.get('n_blanket', 0)\n    obj_sizes_coup = coup_m.get('object_sizes', {})\n    n_internal_coup = sum(obj_sizes_coup.values()) if isinstance(obj_sizes_coup, dict) else 0\n    print(f'  Objects (coupling):   {len(obj_sizes_coup)} ({n_internal_coup} dims)')\n    print(f'  Blanket (coupling):   {n_blanket_coup} dims')\n    \n    hierarchy = dm.get('hierarchy', [])\n    print(f'  Hierarchical levels:  {len(hierarchy)}')\n    \n    # Show Dreamer coupling and eigenvalue figures\n    dreamer_coupling_figs = sorted(RESULTS.glob('*dreamer_analysis_dreamer_coupling_64d.png'))\n    dreamer_eigen_figs = sorted(RESULTS.glob('*dreamer_analysis_dreamer_eigenvalue_spectrum.png'))\n    \n    figs_to_show = []\n    if dreamer_coupling_figs:\n        figs_to_show.append(('64D Latent Coupling Matrix', dreamer_coupling_figs[-1]))\n    if dreamer_eigen_figs:\n        figs_to_show.append(('Eigenvalue Spectrum', dreamer_eigen_figs[-1]))\n    \n    if figs_to_show:\n        from IPython.display import Image, display\n        for title, fpath in figs_to_show:\n            print(f'\\n{title}:')\n            display(Image(filename=str(fpath), width=600))\nelse:\n    print('No Dreamer results found. Run experiments/dreamer_analysis.py first.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Multi-Scale Comparison\n",
    "\n",
    "The same physical system (LunarLander) is analyzed at three representation levels. Do different representations agree on the underlying structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-scale comparison\n",
    "ms_files = sorted(RESULTS.glob('*multi_scale_comparison.json'))\n",
    "if ms_files:\n",
    "    with open(ms_files[-1]) as f:\n",
    "        ms_data = json.load(f)\n",
    "    \n",
    "    mm = ms_data['metrics']\n",
    "    \n",
    "    print('Multi-Scale Comparison: State Space vs Latent Space')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Comparison table\n",
    "    print(f'{\"Representation\":<28s} {\"Dims\":>5s} {\"Objects\":>8s} {\"Blanket\":>8s} {\"Eigengap\":>9s}')\n",
    "    print('-' * 60)\n",
    "    for row in mm['comparison_table']:\n",
    "        print(f'{row[\"name\"]:<28s} {row[\"n_dims\"]:>5d} {row[\"n_objects\"]:>8d} '\n",
    "              f'{row[\"n_blanket\"]:>8d} {row[\"eigengap\"]:>9.1f}')\n",
    "    \n",
    "    # Correspondence\n",
    "    corr = mm['object_correspondence']\n",
    "    nmi = corr['nmi_state_vs_projected_latent']\n",
    "    \n",
    "    print(f'\\nNMI (state-space vs projected latent partition): {nmi:.3f}')\n",
    "    print(f'\\nState-space assignment: {corr[\"state_assignment\"]}')\n",
    "    print(f'Projected latent assignment: {corr[\"projected_latent_assignment\"]}')\n",
    "    \n",
    "    # Latent-to-physical mapping\n",
    "    print(f'\\nLatent blanket maps most strongly to physical variables:')\n",
    "    bp = corr['blanket_physical']\n",
    "    for var, c in zip(bp['top_physical_vars'], bp['top_correlations']):\n",
    "        print(f'  {var}: correlation = {c:.3f}')\n",
    "    \n",
    "    print(f'\\n{mm[\"key_insight\"]}')\n",
    "    \n",
    "    # Show multi-scale figures\n",
    "    ms_figs = sorted(RESULTS.glob('*multi_scale_coupling_comparison.png'))\n",
    "    if ms_figs:\n",
    "        from IPython.display import Image, display\n",
    "        print('\\nCoupling matrices across representations:')\n",
    "        display(Image(filename=str(ms_figs[-1]), width=700))\n",
    "else:\n",
    "    print('No multi-scale results found. Run experiments/multi_scale_comparison.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NMI of ~0.52 indicates partial preservation of structure across representation levels. The latent space encodes the same physical system but distributes it across 64 dimensions, making the blanket structure more diffuse. This is expected: autoencoder latent codes entangle physical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Edge-Compute Implications\n",
    "\n",
    "If TB discovers that a system decomposes into $k$ objects of size $n_i$ with a blanket of size $n_b$, then inference can be *factored*: instead of inverting an $n \\times n$ matrix (cost $\\sim n^2$ for dense products), compute $k$ independent $n_i \\times n_i$ problems plus a blanket update.\n",
    "\n",
    "$$\\text{Speedup} = \\frac{n^2}{\\sum_i n_i^2 + n_b^2 + k \\cdot n_b \\cdot \\max(n_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge-compute analysis\n",
    "edge_files = sorted(RESULTS.glob('*edge_compute_analysis.json'))\n",
    "if edge_files:\n",
    "    with open(edge_files[-1]) as f:\n",
    "        edge_data = json.load(f)\n",
    "    \n",
    "    em = edge_data['metrics']\n",
    "    \n",
    "    print('Edge-Compute Factorization Savings')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    # Specific configurations\n",
    "    configs = ['AI: 3+3+2b', 'Dreamer: 20+20+20+4b', 'Dreamer: 6x10+4b']\n",
    "    print(f'{\"Configuration\":<25s} {\"n\":>5s} {\"Monolithic\":>11s} {\"Factored\":>10s} {\"Speedup\":>8s}')\n",
    "    print('-' * 65)\n",
    "    for cfg_name in configs:\n",
    "        if cfg_name in em:\n",
    "            c = em[cfg_name]\n",
    "            print(f'{cfg_name:<25s} {c[\"n_total\"]:>5d} {c[\"monolithic_flops\"]:>11,d} '\n",
    "                  f'{c[\"factored_flops\"]:>10,d} {c[\"speedup\"]:>7.1f}x')\n",
    "    \n",
    "    # Scaling behavior\n",
    "    print(f'\\nScaling with Dimension:')\n",
    "    print(f'{\"n_total\":>8s} {\"Monolithic\":>12s} {\"Factored\":>12s} {\"Speedup\":>8s} {\"Memory saved\":>13s}')\n",
    "    print('-' * 58)\n",
    "    scaling = em.get('scaling', {})\n",
    "    for dim_key in sorted(scaling.keys(), key=int):\n",
    "        s = scaling[dim_key]\n",
    "        mem_saved = 1.0 - s['factored_flops'] / s['monolithic_flops']\n",
    "        print(f'{s[\"n_total\"]:>8d} {s[\"monolithic_flops\"]:>12,d} {s[\"factored_flops\"]:>12,d} '\n",
    "              f'{s[\"speedup\"]:>7.1f}x {mem_saved:>12.1%}')\n",
    "    \n",
    "    # Speedup curve\n",
    "    dims = [int(k) for k in sorted(scaling.keys(), key=int)]\n",
    "    speedups = [scaling[str(d)]['speedup'] for d in dims]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(dims, speedups, 'o-', color='#2ecc71', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('State Dimension')\n",
    "    ax.set_ylabel('Speedup Factor')\n",
    "    ax.set_title('Factored Inference Speedup vs. Dimension')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    for d, s in zip(dims, speedups):\n",
    "        ax.annotate(f'{s:.1f}x', (d, s), textcoords='offset points',\n",
    "                   xytext=(0, 12), ha='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nAt 4096 dimensions: {speedups[-1]:.1f}x speedup, '\n",
    "          f'{1 - scaling[str(dims[-1])][\"factored_flops\"]/scaling[str(dims[-1])][\"monolithic_flops\"]:.0%} memory savings')\n",
    "else:\n",
    "    print('No edge-compute results found. Run experiments/edge_compute_analysis.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Topological Blankets provides a *geometric* path from continuous energy landscapes to discrete Markov blanket structure:\n",
    "\n",
    "| Experiment | Key Result |\n",
    "|---|---|\n",
    "| Synthetic (quadratic EBM) | ARI $\\geq$ 0.95 when coupling ratio $> 2:1$ |\n",
    "| 2D score model | Successfully extracts Hessian structure from learned score fields |\n",
    "| LunarLander (8D state) | Discovers {position, contact} / {orientation, velocity} split with ang_vel as blanket |\n",
    "| LunarLander (64D latent) | Eigengap=59.8, 40-dim object + 24-dim blanket |\n",
    "| Multi-scale | NMI=0.52 between state-space and projected latent partition |\n",
    "| Edge-compute | 25.9x speedup at 4096D, scaling log-linearly with dimension |\n",
    "\n",
    "The approach requires only gradient samples, works with any differentiable energy function, and runs in seconds for practical dimensionalities. By factoring inference along discovered boundaries, it enables efficient deployment of world models on resource-constrained hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}