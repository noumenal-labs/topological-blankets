\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.97}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codepurple},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python,
}

\title{US-112: BayesformerTB\\
\large TB-Structured Bayesian Transformer with Weight-Space Factor Masking}
\author{Noumenal Labs --- Topological Blankets Project}
\date{February 11, 2026}

\begin{document}

\maketitle

\begin{abstract}
This report documents experiment US-112, which implements a Bayesian transformer
whose attention mechanism is structurally informed by Topological Blankets (TB)
coupling topology. The original implementation contained a critical bug: the TB
attention mask was a no-op because a uniform scalar bias added to attention
logits is softmax-shift-invariant. After diagnosis and correction, the mask was
replaced with a multiplicative gate on the QKV projection weights, where
dimension-level interactions actually occur. The corrected model (BayesformerTB)
achieves CE~3.62, a 21\% improvement over the broken version (CE~4.56), though
it remains above the vanilla GPT-2 baseline (CE~1.73). Coupling sparsity
increases from 0.33 to 0.65 over training, and the loss curve is still declining
steeply at epoch~40, suggesting longer training may close the gap.
\end{abstract}

\tableofcontents

\section{Motivation}

Shai et al.\ (2026) demonstrated that standard transformers learn factored
representations implicitly: attention heads naturally specialize to specific
factors in the data. Topological Blankets (TB) detects this factored structure
by analyzing the coupling topology of the model's residual stream. US-112 asks
whether providing the TB-discovered structure as an architectural inductive bias
can \textit{accelerate} convergence to factored representations.

The hypothesis: if TB provides the structural prior that transformers converge
toward anyway, providing it upfront should reduce the time spent discovering
that structure, allowing the model to focus on learning the factor-conditional
dynamics.

\section{Architecture}

\subsection{Data: 5-Factor GHMM}

The benchmark is a Generalized Hidden Markov Model (GHMM) with 5 independent
factors: 3 Mess3 processes (alphabet size~3) and 2 BlochWalk processes
(alphabet size~4). Tokens are drawn from the Cartesian product
$3 \times 3 \times 3 \times 4 \times 4 = 432$ states (plus a BOS token;
vocab size = 433). Sequences have length~8. The dataset contains 10{,}000
sequences.

\subsection{Model: BayesformerTB}

BayesformerTB matches the SmallGPT2 architecture from US-107:
\begin{itemize}
    \item 4 transformer layers, $d_\text{model} = 120$, $d_\text{MLP} = 480$, 4 attention heads
    \item Weight-tied embedding and output head
    \item Pre-norm (LayerNorm before attention and MLP)
    \item 749{,}644 parameters (vs.\ 749{,}640 for vanilla, the 4-parameter difference is the learned gate logits)
\end{itemize}

The key modifications are:
\begin{enumerate}
    \item \textbf{TB-masked QKV projections.} A soft multiplicative mask on
        the QKV weight matrices that attenuates cross-factor connections while
        preserving within-factor and blanket interactions.
    \item \textbf{Eigengap regularizer.} A loss term that penalizes the ratio
        of cross-object to within-object coupling in the activation covariance.
    \item \textbf{MC Dropout.} Dropout ($p = 0.05$) in attention and MLP for
        Bayesian uncertainty estimation via $K = 10$ Monte Carlo forward passes.
\end{enumerate}

\subsection{TB Factor Mask: Weight-Space Formulation}

The TB partition assigns each of the $d_\text{model} = 120$ residual stream
dimensions to one of $n$ TB objects, or labels it as a blanket variable. The
factor mask $M \in \{0, 1\}^{d_\text{model} \times d_\text{model}}$ encodes
permitted interactions:
\[
    M_{ij} = \begin{cases}
        1 & \text{if } \mathrm{assign}(i) = \mathrm{assign}(j) \geq 0, \\
        1 & \text{if } \mathrm{blanket}(i) \lor \mathrm{blanket}(j), \\
        1 & \text{if } i = j, \\
        0 & \text{otherwise.}
    \end{cases}
\]

This mask is applied to the QKV projection weight matrix
$W_\text{QKV} \in \mathbb{R}^{3d_\text{model} \times d_\text{model}}$ via
a learned gate $g = \sigma(\alpha)$, where $\alpha$ is a per-layer scalar
parameter initialized to 2.0 ($g \approx 0.88$):
\[
    W_\text{eff} = W_\text{QKV} \cdot \bigl(1 - s \cdot (1 - g) \cdot (1 - \tilde{M})\bigr)
\]
where $s \in [0, 1]$ is an external annealing strength and $\tilde{M}$ is
the mask tiled for the Q, K, V blocks:
$\tilde{M} = [M; M; M] \in \mathbb{R}^{3d_\text{model} \times d_\text{model}}$.

This formulation ensures:
\begin{itemize}
    \item When $s = 0$ (warmup): $W_\text{eff} = W_\text{QKV}$ (standard attention).
    \item When $s = 1, g = 1$: $W_\text{eff} = W_\text{QKV}$ (gate fully open, no masking).
    \item When $s = 1, g = 0$: $W_\text{eff} = W_\text{QKV} \cdot \tilde{M}$ (full masking).
\end{itemize}

The gate is differentiable and can be learned jointly with the model weights.

\section{The Bug: Why the Original Mask Was a No-Op}

\subsection{Original Implementation}

The original code applied the TB mask as a scalar additive bias to attention
scores. For each head $h$ with factor mask
$M_h \in \mathbb{R}^{d_\text{head} \times d_\text{head}}$:
\begin{lstlisting}[style=pythonstyle]
mask_bias = head_mask.mean()  # scalar!
scores[:, h, :, :] += alpha * mask_bias
\end{lstlisting}

This collapses the full $d_\text{head} \times d_\text{head}$ structural
information into a single scalar, then adds it uniformly to all sequence-position
attention scores for that head.

\subsection{Why This Is a No-Op}

The softmax function is shift-invariant:
\[
    \mathrm{softmax}(x + c) = \mathrm{softmax}(x) \quad \forall\, c \in \mathbb{R}
\]
Adding a constant $\alpha \cdot \bar{M}_h$ to every element of
$\text{scores}_{:, h, :, :}$ does not change the attention weights. The
causal mask (which sets some entries to $-\infty$) does not break this
invariance, because $-\infty + c = -\infty$.

Therefore, the TB mask had \textit{zero effect} on the forward pass. The
entire performance gap in the original run (CE~4.56 vs.\ 1.77) was
attributable to MC Dropout noise ($p = 0.1$), not the structural prior.

\subsection{The Conceptual Mismatch}

The deeper issue was a confusion between two axes of interaction:
\begin{itemize}
    \item \textbf{Sequence axis:} attention scores operate over sequence
        positions (which tokens attend to which).
    \item \textbf{Dimension axis:} the TB factor mask operates over residual
        stream dimensions (which features interact with which).
\end{itemize}

Standard multi-head attention computes interactions in sequence space. The
QKV projection weights are where dimension-space interactions occur. The
mask must be applied to the weights, not to the scores.

\section{Training Protocol}

\begin{enumerate}
    \item \textbf{Epochs 1--5 (warmup):} No TB mask. Standard attention.
        The model develops initial representations.
    \item \textbf{Epoch 6:} First TB analysis on layer\_2 residual stream.
        Factor mask constructed and applied. Mask strength $s = 0$.
    \item \textbf{Epochs 6--20:} Mask strength annealed linearly from 0 to 1
        over 15 epochs (curriculum learning).
    \item \textbf{Every 5 epochs:} TB re-analysis; mask and regularizer
        partition updated.
    \item \textbf{Eigengap regularizer:} Active from epoch 6 onward,
        $\lambda_\text{reg} = 0.005$.
\end{enumerate}

\section{Results}

\subsection{Loss Convergence}

\begin{table}[h]
\centering
\caption{Final cross-entropy loss after 40 epochs of training.}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Final CE} & \textbf{Train Time (s)} & \textbf{Params} & \textbf{Dropout} \\
\midrule
BayesformerTB (broken, v1)  & 4.5599 & 913.8 & 749{,}644 & 0.10 \\
BayesformerTB (fixed, v2)   & 3.6221 & 576.6 & 749{,}644 & 0.05 \\
Vanilla GPT-2               & 1.7264 & 157.2 & 749{,}640 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

The fix reduced BayesformerTB loss by 21\% (4.56 $\to$ 3.62) and training
time by 37\% (914s $\to$ 577s; the reduced dropout decreases the effective
noise per gradient step). However, vanilla GPT-2 still converges
substantially faster.

\subsection{Loss Curve Analysis}

Both models track together through epoch $\sim$12, after which vanilla pulls
ahead. BayesformerTB's loss is still declining steeply at epoch 40 (the
epoch 35$\to$40 drop is $4.02 \to 3.62$, roughly 0.08/epoch), while vanilla
has essentially plateaued ($1.77 \to 1.73$). Extrapolating the BayesformerTB
trajectory suggests it would need $\sim$80--100 additional epochs to reach
the vanilla baseline.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{us112_training_curves.png}
\caption{Training loss curves for BayesformerTB (fixed) and vanilla GPT-2.
Left: cross-entropy loss over 40 epochs. Right: eigengap regularization loss.}
\label{fig:curves}
\end{figure}

The eigengap regularization loss (right panel of Figure~\ref{fig:curves})
spikes when the TB mask activates at epoch~6, then decays as the model
adapts its representations to satisfy both the CE and structural objectives.

\subsection{Eigengap Trajectory}

\begin{table}[h]
\centering
\caption{Layer\_2 eigengap at selected epochs.}
\label{tab:eigengap}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Epoch 5} & \textbf{Epoch 20} & \textbf{Epoch 40} & \textbf{First $\geq 5$} \\
\midrule
BayesformerTB & 110.57 & 103.92 & 96.43 & Epoch 5 \\
Vanilla GPT-2 & 110.58 & 100.08 & 93.93 & Epoch 5 \\
\bottomrule
\end{tabular}
\end{table}

Both models achieve eigengap $\geq 5$ at the first checkpoint (epoch~5).
The eigengap is already very high at initialization, reflecting the GHMM's
strong factored structure. This benchmark may be too ``easy'' for the
structural prior to provide measurable advantage.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{us112_eigengap_trajectory.png}
\caption{Layer\_2 eigengap trajectory over training for BayesformerTB and
vanilla GPT-2. Both models reach high eigengap early, reflecting the
strong factored structure of the GHMM benchmark.}
\label{fig:eigengap}
\end{figure}

\subsection{Per-Factor Uncertainty}

BayesformerTB provides calibrated per-factor uncertainty via MC Dropout.
The total predictive uncertainty is $1.04 \times 10^{-4}$ (vs.\
$1 \times 10^{-8}$ for vanilla's noise-based proxy). Per-factor marginal
entropies range from 1.02 to 1.27 nats across the 5 factors, providing
factor-specific confidence estimates that vanilla cannot produce.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{us112_per_factor_uncertainty.png}
\caption{Per-factor marginal entropy estimates from BayesformerTB's MC
Dropout inference ($K = 10$ samples). Each bar represents the predictive
entropy for one of the 5 GHMM factors, enabling factor-specific confidence
assessment.}
\label{fig:uncertainty}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{us112_attention_mask_evolution.png}
\caption{Evolution of the TB weight-space mask across training. The mask
strength anneals from 0 to 1 over 15 epochs after the warmup period,
progressively enforcing the factor structure discovered by TB analysis.}
\label{fig:mask_evolution}
\end{figure}

\section{Discussion}

\subsection{Why Vanilla Still Wins on This Benchmark}

Three factors contribute to the remaining gap:

\begin{enumerate}
    \item \textbf{Task simplicity.} The 5-factor GHMM has strong factored
        structure that vanilla discovers within $\sim$10 epochs. The
        structural prior cannot ``help'' discover something the model already
        finds easily.
    \item \textbf{MC Dropout cost.} Even at $p = 0.05$, dropout adds noise
        to every forward pass during training, slowing convergence. Vanilla
        has no dropout.
    \item \textbf{Double forward pass.} The eigengap regularizer requires a
        second forward pass (with residual stream extraction) per batch,
        adding computational overhead and potentially interfering with
        gradient dynamics.
\end{enumerate}

\subsection{Where BayesformerTB Should Excel}

The structural prior is expected to provide advantage in settings where:
\begin{itemize}
    \item The factored structure is harder to discover (more factors, weaker
        inter-factor independence, longer sequences).
    \item Per-factor uncertainty is decision-relevant (e.g., teleoperation
        handoff decisions based on which factor is uncertain).
    \item Training data is limited and the structural prior acts as an
        effective regularizer.
\end{itemize}

\section{File Locations}

\begin{table}[h]
\centering
\small
\caption{Experiment artifacts and their file paths (relative to the
\texttt{topological-blankets} repository root).}
\label{tab:files}
\begin{tabular}{p{0.35\linewidth}p{0.55\linewidth}}
\toprule
\textbf{Artifact} & \textbf{Path} \\
\midrule
Experiment script & \texttt{ralph/experiments/bayesformer\_tb.py} \\
Results JSON & \texttt{ralph/results/us112\_bayesformer\_tb.json} \\
Training curves plot & \texttt{ralph/results/us112\_training\_curves.png} \\
Eigengap trajectory plot & \texttt{ralph/results/us112\_eigengap\_trajectory.png} \\
Per-factor uncertainty plot & \texttt{ralph/results/us112\_per\_factor\_uncertainty.png} \\
Attention mask evolution & \texttt{ralph/results/us112\_attention\_mask\_evolution.png} \\
PRD entry & \texttt{ralph/prd.json} (US-112) \\
GHMM data generator & \texttt{ralph/experiments/fwh\_ghmm\_tb\_detection.py} \\
TB library & \texttt{topological\_blankets/} \\
\bottomrule
\end{tabular}
\end{table}

\section{Configuration}

\begin{table}[h]
\centering
\caption{Full experiment configuration.}
\label{tab:config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Sequences & 10{,}000 \\
Sequence length & 8 \\
Epochs & 40 \\
Batch size & 256 \\
Learning rate & $1 \times 10^{-3}$ (Adam) \\
Warmup epochs & 5 \\
TB update frequency & Every 5 epochs \\
Mask anneal epochs & 15 \\
$\lambda_\text{reg}$ & 0.005 \\
Dropout $p$ & 0.05 \\
MC samples $K$ & 10 \\
TB method & Hybrid \\
Expected objects & 5 \\
\bottomrule
\end{tabular}
\end{table}

\section{Reproduction}

\begin{lstlisting}[style=pythonstyle,caption={Running the experiment.}]
cd topological-blankets
python ralph/experiments/bayesformer_tb.py
\end{lstlisting}

The script generates all plots, the results JSON, and prints a summary with
acceptance criteria status. Requires PyTorch, NumPy, matplotlib, and the
\texttt{topological\_blankets} package (included in the repository).

\end{document}
